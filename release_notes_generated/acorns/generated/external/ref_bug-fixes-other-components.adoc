[id="bug-fixes-other-components"]
= Other components

I didn't know how exactly to sort these.


[id="BZ-2099991"]
* By default, Buildah prints steps to the log file, including the contents of environment variables, which might include xref:../cicd/builds/creating-build-inputs.adoc#builds-input-secrets-configmaps_creating-build-inputs[build input secrets]. Although you can use the `--quiet` build argument to suppress printing of those environment variables, this argument isn't available if you use the source-to-image (S2I) build strategy. The current release fixes this issue. To suppress printing of environment variables, set the `BUILDAH_QUIET` environment variable in your build configuration:
+
[source,yaml]
----
sourceStrategy:
...
  env:
    - name: "BUILDAH_QUIET"
      value: "true"
----

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2099991[Bugzilla:2099991]) 

[id="BZ-1997396"]
* Previously, the cluster autoscaler metrics for cluster CPU and memory usage would never reach, or exceed, the limits set by the `ClusterAutoscaler` resource. As a result, no alerts were fired when the cluster autoscaler could not scale due to resource limitations. With this release, a new metric called `cluster_autoscaler_skipped_scale_events_count` is added to the cluster autoscaler to more accurately detect when resource limits are reached or exceeded. Alerts will now fire when the cluster autoscaler is unable to scale the cluster up because it has reached the cluster resource limits. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1997396[*BZ#1997396*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=1997396[Bugzilla:1997396]) 

[id="BZ-2001027"]
* Previously, the cluster autoscaler did not respect the AWS, IBM Cloud, and Alibaba Cloud topology labels for the CSI drivers when using the Cluster API provider. As a result, nodes with the topology label were not processed properly by the autoscaler when attempting to balance nodes during a scale-out event. With this release, the autoscaler's custom processors are updated so that it respects this label. The autoscaler can now balance similar node groups that are labelled by the the AWS, IBM Cloud, or Alibaba CSI labels. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001027[*BZ#2001027*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2001027[Bugzilla:2001027]) 

[id="BZ-2060068"]
* Previously, the Machine API provider for AWS did not verify that the security group defined in the machine specification exists. Instead of returning an error in this case, it used a default security group, which should not be used for {product-title} machines, and successfully created a machine without informing the user that the default group was used. With this release, the Machine API returns an error when users set either incorrect or empty security group names in the machine specification. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2060068[*BZ#2060068*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2060068[Bugzilla:2060068]) 

[id="BZ-2062579"]
* Previously, when creating a new `Machine` resource using a machine profile that does not exist in IBM Cloud, the machines became stuck in the `Provisioning` phase. With this release, validation is added to the IBM Cloud Machine API provider to ensure that a machine profile exists, an machines with an invalid machine profile are rejected by the Machine API. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2062579[*BZ#2062579*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2062579[Bugzilla:2062579]) 

[id="BZ-2085390"]
* Previously, the Machine API provider Azure did not treated user-provided values for instance types as case sensitive. This led to false-positive errors when instance types were correct but did not match the case. With this release, instance types are converted to the lower case so that users get correct results without false-positive errors for mismatched case. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2085390[*BZ#2085390*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2085390[Bugzilla:2085390]) 

[id="BZ-2087981"]
* Previously, the Machine API vSphere machine controller set the `PowerOn` flag when cloning a VM. This created a `PowerOn` task that the machine controller was not aware of. If that `PowerOn` task failed, machines were stuck in the `Provisioned` phase but never powered on. With this release, the cloning sequence is altered to avoid the issue. Additionally, the machine controller now retries powering on the VM in case of failure and reports failures properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2087981[*BZ#2087981*], link:https://issues.redhat.com/browse/OCPBUGS-954[*OCPBUGS-954*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2087981[Bugzilla:2087981]) 

[id="BZ-2101736"]
* Previously, machines created in early versions of {product-title} with invalid configurations could not be deleted. With this release, the webhooks that prevent the creation of machines with invalid configurations no longer prevent the deletion of existing invalid machines. Users can now successfully remove these machines from their cluster by manually removing the finalizers on these machines. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2101736[*BZ#2101736*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2101736[Bugzilla:2101736]) 

[id="BZ-2102383"]
* Previously, a bug in the {rh-openstack} legacy cloud provider resulted in a crash if certain {rh-openstack} operations were attempted after authentication had failed. For example, shutting down a server causes the Kubernetes controller manager to fetch server information from {rh-openstack}, which triggered this bug. As a result, if initial cloud authentication failed or was configured incorrectly, shutting down a server caused the Kubernetes controller manager to crash. With this release, the {rh-openstack} legacy cloud provider is updated to not attempt any {rh-openstack} API calls if it has not previously authenticated successfully. Now, shutting down a server with invalid cloud credentials no longer causes Kubernetes controller manager to crash.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102383[*BZ#2102383*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102383[Bugzilla:2102383]) 

[id="BZ-2104373"]
* Previously, the Cloud Controller Manager Operator did not check the `cloud-config` configuration file for AWS clusters. As a result, it was not possible to pass additional settings to the AWS cloud controller manager component by using the configuration file. With this release, the Cloud Controller Manager Operator checks the infrastructure resource and parses references to the `cloud-config` configuration file so that users can configure additional settings. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2104373[*BZ#2104373*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2104373[Bugzilla:2104373]) 

[id="BZ-2106733"]
* Previously, there was no check for nil values in the annotations of a machine object before attempting to access the object. This situation was rare, but caused the machine controller to panic when reconciling the machine. With this release, nil values are checked and the machine controller is able to reconcile machines without annotations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2106733[*BZ#2106733*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2106733[Bugzilla:2106733]) 

[id="BZ-2108647"]
* Previously, when Azure added new instance types and enabled accelerated networking support on instance types that previously did not have it, the list of Azure instances in the machine controller became outdated. As a result, the machine controller could not create machines with instance types that did not previously support accelerated networking, even if they support this feature on Azure. With this release, the required instance type information is retrieved from Azure API before the machine is created to keep it up to date and the machine controller is able to create machines with new and updated instance types. This fix also applies to any instance types that are added in the future. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2108647[*BZ#2108647*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2108647[Bugzilla:2108647]) 

[id="BZ-2111467"]
* Previously, when the Machine API provider failed to fetch the machine IP address, it would not set the internal DNS name and the machine certificate signing requests were not automatically approved. With this release, the Power VS machine provider is updated to set the server name as the internal DNS name even when it fails to fetch the IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2111467[*BZ#2111467*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2111467[Bugzilla:2111467]) 

[id="BZ-2111474"]
* Previously, Power VS cloud providers were not capable of fetching the machine IP address from DHCP server. Changing the IP address did not update the node, which caused some inconsistencies, such as pending certificate signing requests. With this release, the Power VS cloud provider is updated to fetch the machine IP address from the DHCP server so that the IP addresses for the nodes are consistent with the machine IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2111474[*BZ#2111474*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2111474[Bugzilla:2111474]) 

[id="BZ-2115090"]
* Previously, short DHCP lease times in combination with `NetworkManager` not being run as a daemon or in continuous mode caused machines to become stuck during their ignition configuration never become nodes in the cluster. With this release, extra checks are added so that if a machine becomes stuck in this state it is deleted and recreated  automatically. Machines that are affected by this network condition can become nodes after a reboot from the Machine API controller. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2115090[*BZ#2115090*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2115090[Bugzilla:2115090]) 

[id="BZ-2051443"]
* Previously, the `oslat` runner configured `oslat` to use all available CPUs, which caused false spikes. With this update, the `oslat` runner reserves one CPU for the control thread. As a result, false spikes no longer occur. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2051443[*BZ#2051443*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2051443[Bugzilla:2051443]) 

[id="BZ-2060726"]
* Previously, the Compliance Operator hard-coded notifications to the default namespace. As a result, notifications from the Operator would not appear if the Operator was installed in a different namespace. This issue is fixed in this release.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2060726[*BZ#2060726*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2060726[Bugzilla:2060726]) 

[id="BZ-2094382"]
* Previously, applying automatic remediation for the `rhcos4-high-master-sysctl-kernel-yama-ptrace-scope` and `rhcos4-sysctl-kernel-core-pattern` rules resulted in subsequent failures of those rules in scan results, even though they were remediated. The issue is fixed in this release.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2094382[*BZ#2094382*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2094382[Bugzilla:2094382]) 

[id="BZ-2098581"]
* Previously, the Compliance Operator used an old version of the Operator SDK, which is a dependency for building Operators. This caused alerts about deprecated Kubernetes functionality used by the Operator SDK. With this release, the Compliance Operator is upgraded to version 0.1.55, which includes an updated version of the Operator SDK.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2098581[*BZ#2098581*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2098581[Bugzilla:2098581]) 

[id="BZ-2102511"]
* Previously, the Compliance Operator held machine configurations in a stuck state because it could not determine the relationship between machine configurations and kubelet configurations due to incorrect assumptions about machine configuration names. With this release, the Compliance Operator is able to determine if a kubelet configuration is a subset of a machine configuration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102511[*BZ#2102511*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102511[Bugzilla:2102511]) 

[id="BZ-2117268"]
* Previously, the Compliance Operator failed to fetch API resources when parsing machine configurations without ignition specifications. This caused the `api-check-pods` check to crash loop. With this release, the Compliance Operator is updated to gracefully handle machine config pools without ignition specifications.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2117268[*BZ#2117268*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2117268[Bugzilla:2117268]) 

[id="BZ-2071792"]
* Previously, the `openshift-config` namespace was hardcoded for the `HelmChartRepository` custom resource, which was the same namespace for the `ProjectHelmChartRepository` custom resource. This prevent users from adding private `ProjectHelmChartRepository` custom resources in their desired namespace. Consquently, users were unable to access secrets and configmaps in the `openshift-config` namespace. This update fixes the project Helm chart repository custom resource definition with a namespace field that can read the secret and configmaps from a namespace of choice by a user with the correct permissions. Additionally, the user can add secrets and configmaps to the accessible namespace, and they can add private Helm cart repositories in the namespace used the creation resources. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2071792[*BZ#2071792*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2071792[Bugzilla:2071792]) 

[id="BZ-2089221"]
* Previously, the users could not deselect a Git secret in add and edit forms. As a result, the resources had to be recreated. This fix resolves the issue by adding the option to choose `No Secret` in the select secret option list. As a result, the users can easily select, deselect, or detach any attached secrets.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=2089221[*BZ#2089221*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2089221[Bugzilla:2089221]) 

[id="BZ-2101393"]
* Previously, alerts issued by the File Integrity Operator did not set a namespace. This made it difficult to understand where the alert was coming from, or what component was responsible for issuing it. With this release, the Operator includes the namespace it was installed into in the alert, making it easier to narrow down what component needs attention.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2101393[*2101393*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2101393[Bugzilla:2101393]) 

[id="BZ-2104897"]
* Previously, the File Integrity Operator deployed templates using the `openshift-file-integrity` namespace in the permissions for the Operator. When the Operator attempted to create objects in the namespace, to would fail due to permission issues. With this release, the deployment resources used by OLM are updated to use the correct namespace, fixing the permission issues so that users can install and use the operator in a non-default namespaces.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2104897[*BZ#2104897*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2104897[Bugzilla:2104897]) 

[id="BZ-2108475"]
* Previously, the File Integrity Operator daemon used the `ClusterRoles` parameter instead of the `Roles` parameter for a recent permission change. As a result, OLM could not upgrade the Operator. With this release, the Operator daemon reverts to using the `Roles` parameter and upgrades from older versions to version 0.1.29 are successful.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2108475[*BZ#2108475*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2108475[Bugzilla:2108475]) 

[id="BZ-2109153"]
* Previously, service account ownership for the File Integrity Operator regressed due to underlying OLM updates, and updates from 0.1.24 to 0.1.29 were broken. With this update, the Operator should default to upgrading to 0.1.30. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2109153[*BZ#2109153*]

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2109153[Bugzilla:2109153]) 

[id="BZ-2112394"]
* Previously, the File Integrity Operator did not properly handle modifying alerts during an upgrade. As a result, alerts did not include the namespace in which the Operator was installed. With this release, the Operator includes the namespace it was installed into in the alert, making it easier to narrow down what component needs attention.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2112394[*2112394*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2112394[Bugzilla:2112394]) 

[id="BZ-2115821"]
* Previously, underlying dependencies of the File Integrity Operator changed how alerts and notifications were handled, and the Operator didn't send metrics as a result. With this release the Operator ensures that the metrics endpoint is correct and reachable on startup.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2115821[*2115821*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2115821[Bugzilla:2115821]) 

[id="BZ-2118286"]
* Previously, the `kube-controller-manager` Operator was reporting `degraded` on environments without a monitoring stack presence. With this update, the `kube-controller-manager` Operator skips checking the monitoring for cues about degradation when the monitoring stack is not present. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2118286[*BZ#2118286*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2118286[Bugzilla:2118286]) 

[id="BZ-2100923"]
* Previously, the secondary scheduler deployment was not deleted after a secondary scheduler custom resource was deleted. Consequently, the Secondary Schedule Operator and Operand were not fully uninstalled. With this update, the correct owner reference is set in the secondary scheduler custom resource so that it points to the secondary scheduler deployment. As a result, secondary scheduler deployments are deleted when the secondary scheduler custom resource is deleted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2100923[*BZ#2100923*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2100923[Bugzilla:2100923]) 

[id="BZ-2107261"]
* Previously, restarting the Windows Machine Config Operator (WMCO) in a cluster with running Windows nodes caused the Windows exporter endpoint to be removed. Because of this, each Windows node could not report any metrics data. With this update, the endpoint is retained when the WMCO is started. As a result, metrics data is reported properly after restarting WMCO. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2107261[*BZ#2107261*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2107261[Bugzilla:2107261]) 
