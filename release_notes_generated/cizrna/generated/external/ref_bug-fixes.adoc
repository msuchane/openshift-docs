[id="bug-fixes"]
= Bug fixes

This part describes bugs fixed in {ProductName}{nbsp}{ProductNumber} that have a significant impact on users.


[id="BZ-2099991"]
* By default, Buildah prints steps to the log file, including the contents of environment variables, which might include xref:../cicd/builds/creating-build-inputs.adoc#builds-input-secrets-configmaps_creating-build-inputs[build input secrets]. Although you can use the `--quiet` build argument to suppress printing of those environment variables, this argument isn't available if you use the source-to-image (S2I) build strategy. The current release fixes this issue. To suppress printing of environment variables, set the `BUILDAH_QUIET` environment variable in your build configuration:
+
[source,yaml]
----
sourceStrategy:
...
  env:
    - name: "BUILDAH_QUIET"
      value: "true"
----

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2099991[Bugzilla:2099991]) 

[id="BZ-1997396"]
* Previously, the cluster autoscaler metrics for cluster CPU and memory usage would never reach, or exceed, the limits set by the `ClusterAutoscaler` resource. As a result, no alerts were fired when the cluster autoscaler could not scale due to resource limitations. With this release, a new metric called `cluster_autoscaler_skipped_scale_events_count` is added to the cluster autoscaler to more accurately detect when resource limits are reached or exceeded. Alerts will now fire when the cluster autoscaler is unable to scale the cluster up because it has reached the cluster resource limits. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1997396[*BZ#1997396*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=1997396[Bugzilla:1997396]) 

[id="BZ-2001027"]
* Previously, the cluster autoscaler did not respect the AWS, IBM Cloud, and Alibaba Cloud topology labels for the CSI drivers when using the Cluster API provider. As a result, nodes with the topology label were not processed properly by the autoscaler when attempting to balance nodes during a scale-out event. With this release, the autoscaler's custom processors are updated so that it respects this label. The autoscaler can now balance similar node groups that are labelled by the the AWS, IBM Cloud, or Alibaba CSI labels. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001027[*BZ#2001027*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2001027[Bugzilla:2001027]) 

[id="BZ-2060068"]
* Previously, the Machine API provider for AWS did not verify that the security group defined in the machine specification exists. Instead of returning an error in this case, it used a default security group, which should not be used for {product-title} machines, and successfully created a machine without informing the user that the default group was used. With this release, the Machine API returns an error when users set either incorrect or empty security group names in the machine specification. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2060068[*BZ#2060068*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2060068[Bugzilla:2060068]) 

[id="BZ-2062579"]
* Previously, when creating a new `Machine` resource using a machine profile that does not exist in IBM Cloud, the machines became stuck in the `Provisioning` phase. With this release, validation is added to the IBM Cloud Machine API provider to ensure that a machine profile exists, an machines with an invalid machine profile are rejected by the Machine API. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2062579[*BZ#2062579*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2062579[Bugzilla:2062579]) 

[id="BZ-2085390"]
* Previously, the Machine API provider Azure did not treated user-provided values for instance types as case sensitive. This led to false-positive errors when instance types were correct but did not match the case. With this release, instance types are converted to the lower case so that users get correct results without false-positive errors for mismatched case. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2085390[*BZ#2085390*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2085390[Bugzilla:2085390]) 

[id="BZ-2087981"]
* Previously, the Machine API vSphere machine controller set the `PowerOn` flag when cloning a VM. This created a `PowerOn` task that the machine controller was not aware of. If that `PowerOn` task failed, machines were stuck in the `Provisioned` phase but never powered on. With this release, the cloning sequence is altered to avoid the issue. Additionally, the machine controller now retries powering on the VM in case of failure and reports failures properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2087981[*BZ#2087981*], link:https://issues.redhat.com/browse/OCPBUGS-954[*OCPBUGS-954*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2087981[Bugzilla:2087981]) 

[id="BZ-2101736"]
* Previously, machines created in early versions of {product-title} with invalid configurations could not be deleted. With this release, the webhooks that prevent the creation of machines with invalid configurations no longer prevent the deletion of existing invalid machines. Users can now successfully remove these machines from their cluster by manually removing the finalizers on these machines. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2101736[*BZ#2101736*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2101736[Bugzilla:2101736]) 

[id="BZ-2102383"]
* Previously, a bug in the {rh-openstack} legacy cloud provider resulted in a crash if certain {rh-openstack} operations were attempted after authentication had failed. For example, shutting down a server causes the Kubernetes controller manager to fetch server information from {rh-openstack}, which triggered this bug. As a result, if initial cloud authentication failed or was configured incorrectly, shutting down a server caused the Kubernetes controller manager to crash. With this release, the {rh-openstack} legacy cloud provider is updated to not attempt any {rh-openstack} API calls if it has not previously authenticated successfully. Now, shutting down a server with invalid cloud credentials no longer causes Kubernetes controller manager to crash.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102383[*BZ#2102383*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102383[Bugzilla:2102383]) 

[id="BZ-2104373"]
* Previously, the Cloud Controller Manager Operator did not check the `cloud-config` configuration file for AWS clusters. As a result, it was not possible to pass additional settings to the AWS cloud controller manager component by using the configuration file. With this release, the Cloud Controller Manager Operator checks the infrastructure resource and parses references to the `cloud-config` configuration file so that users can configure additional settings. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2104373[*BZ#2104373*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2104373[Bugzilla:2104373]) 

[id="BZ-2106733"]
* Previously, there was no check for nil values in the annotations of a machine object before attempting to access the object. This situation was rare, but caused the machine controller to panic when reconciling the machine. With this release, nil values are checked and the machine controller is able to reconcile machines without annotations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2106733[*BZ#2106733*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2106733[Bugzilla:2106733]) 

[id="BZ-2108647"]
* Previously, when Azure added new instance types and enabled accelerated networking support on instance types that previously did not have it, the list of Azure instances in the machine controller became outdated. As a result, the machine controller could not create machines with instance types that did not previously support accelerated networking, even if they support this feature on Azure. With this release, the required instance type information is retrieved from Azure API before the machine is created to keep it up to date and the machine controller is able to create machines with new and updated instance types. This fix also applies to any instance types that are added in the future. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2108647[*BZ#2108647*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2108647[Bugzilla:2108647]) 

[id="BZ-2111467"]
* Previously, when the Machine API provider failed to fetch the machine IP address, it would not set the internal DNS name and the machine certificate signing requests were not automatically approved. With this release, the Power VS machine provider is updated to set the server name as the internal DNS name even when it fails to fetch the IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2111467[*BZ#2111467*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2111467[Bugzilla:2111467]) 

[id="BZ-2111474"]
* Previously, Power VS cloud providers were not capable of fetching the machine IP address from DHCP server. Changing the IP address did not update the node, which caused some inconsistencies, such as pending certificate signing requests. With this release, the Power VS cloud provider is updated to fetch the machine IP address from the DHCP server so that the IP addresses for the nodes are consistent with the machine IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2111474[*BZ#2111474*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2111474[Bugzilla:2111474]) 

[id="BZ-2115090"]
* Previously, short DHCP lease times in combination with `NetworkManager` not being run as a daemon or in continuous mode caused machines to become stuck during their ignition configuration never become nodes in the cluster. With this release, extra checks are added so that if a machine becomes stuck in this state it is deleted and recreated  automatically. Machines that are affected by this network condition can become nodes after a reboot from the Machine API controller. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2115090[*BZ#2115090*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2115090[Bugzilla:2115090]) 

[id="BZ-2051443"]
* Previously, the `oslat` runner configured `oslat` to use all available CPUs, which caused false spikes. With this update, the `oslat` runner reserves one CPU for the control thread. As a result, false spikes no longer occur. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2051443[*BZ#2051443*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2051443[Bugzilla:2051443]) 

[id="BZ-2060726"]
* Previously, the Compliance Operator hard-coded notifications to the default namespace. As a result, notifications from the Operator would not appear if the Operator was installed in a different namespace. This issue is fixed in this release.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2060726[*BZ#2060726*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2060726[Bugzilla:2060726]) 

[id="BZ-2094382"]
* Previously, applying automatic remediation for the `rhcos4-high-master-sysctl-kernel-yama-ptrace-scope` and `rhcos4-sysctl-kernel-core-pattern` rules resulted in subsequent failures of those rules in scan results, even though they were remediated. The issue is fixed in this release.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2094382[*BZ#2094382*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2094382[Bugzilla:2094382]) 

[id="BZ-2098581"]
* Previously, the Compliance Operator used an old version of the Operator SDK, which is a dependency for building Operators. This caused alerts about deprecated Kubernetes functionality used by the Operator SDK. With this release, the Compliance Operator is upgraded to version 0.1.55, which includes an updated version of the Operator SDK.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2098581[*BZ#2098581*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2098581[Bugzilla:2098581]) 

[id="BZ-2102511"]
* Previously, the Compliance Operator held machine configurations in a stuck state because it could not determine the relationship between machine configurations and kubelet configurations due to incorrect assumptions about machine configuration names. With this release, the Compliance Operator is able to determine if a kubelet configuration is a subset of a machine configuration.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102511[*BZ#2102511*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102511[Bugzilla:2102511]) 

[id="BZ-2117268"]
* Previously, the Compliance Operator failed to fetch API resources when parsing machine configurations without ignition specifications. This caused the `api-check-pods` check to crash loop. With this release, the Compliance Operator is updated to gracefully handle machine config pools without ignition specifications.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2117268[*BZ#2117268*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2117268[Bugzilla:2117268]) 

[id="BZ-2071792"]
* Previously, the `openshift-config` namespace was hardcoded for the `HelmChartRepository` custom resource, which was the same namespace for the `ProjectHelmChartRepository` custom resource. This prevent users from adding private `ProjectHelmChartRepository` custom resources in their desired namespace. Consquently, users were unable to access secrets and configmaps in the `openshift-config` namespace. This update fixes the project Helm chart repository custom resource definition with a namespace field that can read the secret and configmaps from a namespace of choice by a user with the correct permissions. Additionally, the user can add secrets and configmaps to the accessible namespace, and they can add private Helm cart repositories in the namespace used the creation resources. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2071792[*BZ#2071792*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2071792[Bugzilla:2071792]) 

[id="BZ-2089221"]
* Previously, the users could not deselect a Git secret in add and edit forms. As a result, the resources had to be recreated. This fix resolves the issue by adding the option to choose `No Secret` in the select secret option list. As a result, the users can easily select, deselect, or detach any attached secrets.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=2089221[*BZ#2089221*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2089221[Bugzilla:2089221]) 

[id="BZ-2101393"]
* Previously, alerts issued by the File Integrity Operator did not set a namespace. This made it difficult to understand where the alert was coming from, or what component was responsible for issuing it. With this release, the Operator includes the namespace it was installed into in the alert, making it easier to narrow down what component needs attention.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2101393[*2101393*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2101393[Bugzilla:2101393]) 

[id="BZ-2104897"]
* Previously, the File Integrity Operator deployed templates using the `openshift-file-integrity` namespace in the permissions for the Operator. When the Operator attempted to create objects in the namespace, to would fail due to permission issues. With this release, the deployment resources used by OLM are updated to use the correct namespace, fixing the permission issues so that users can install and use the operator in a non-default namespaces.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2104897[*BZ#2104897*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2104897[Bugzilla:2104897]) 

[id="BZ-2108475"]
* Previously, the File Integrity Operator daemon used the `ClusterRoles` parameter instead of the `Roles` parameter for a recent permission change. As a result, OLM could not upgrade the Operator. With this release, the Operator daemon reverts to using the `Roles` parameter and upgrades from older versions to version 0.1.29 are successful.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2108475[*BZ#2108475*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2108475[Bugzilla:2108475]) 

[id="BZ-2109153"]
* Previously, service account ownership for the File Integrity Operator regressed due to underlying OLM updates, and updates from 0.1.24 to 0.1.29 were broken. With this update, the Operator should default to upgrading to 0.1.30. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2109153[*BZ#2109153*]

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2109153[Bugzilla:2109153]) 

[id="BZ-2112394"]
* Previously, the File Integrity Operator did not properly handle modifying alerts during an upgrade. As a result, alerts did not include the namespace in which the Operator was installed. With this release, the Operator includes the namespace it was installed into in the alert, making it easier to narrow down what component needs attention.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2112394[*2112394*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2112394[Bugzilla:2112394]) 

[id="BZ-2115821"]
* Previously, underlying dependencies of the File Integrity Operator changed how alerts and notifications were handled, and the Operator didn't send metrics as a result. With this release the Operator ensures that the metrics endpoint is correct and reachable on startup.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2115821[*2115821*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2115821[Bugzilla:2115821]) 

[id="BZ-2055620"]
* Previously, the image trigger controller did not have permissions to change objects. Consequently, image trigger annotations did not work on some resources. This update creates cluster role binding that provides the controller the required permissions to update objects according to annotations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055620([*BZ#2055620*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2055620[Bugzilla:2055620]) 

[id="BZ-2093440"]
* Previously, the Image Registry Operator did not have a `progressing` condition for the `node-ca` daemon set and used `generation` from an incorrect object. Consequently, the `node-ca` daemon set could be marked as `degraded` while the Operator was still running. This update adds the `progressing` condition, which indicates that the installation is not complete. As a result, the Image Registry Operator successfully installs the `node-ca` daemon set and the installer waits until it is fully deployed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2093440[(*BZ#2093440*)]

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2093440[Bugzilla:2093440]) 

[id="BZ-1944365"]
* Previously, users could manually set the API and Ingress virtual IP addresses to values that conflicted with the allocation pool of the DHCP server when installing a cluster on OpenStack. This could cause the DHCP server to assign one of the VIP addresses to a new machine, which would fail to start. In this update, the installation program validates the user-provided VIP addresses to ensure that they do not conflict with any DHCP pools. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1944365[*BZ1944365*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=1944365[Bugzilla:1944365]) 

[id="BZ-2055247"]
* Previously, installing a cluster on Microsoft Azure failed when the Azure DCasv5-series or DCadsv5-series of confidential VMs were specified as control plane nodes. With this update, the installation program now stops the installation with an error, which states that confidential VMs are not yet supported. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055247[*BZ#2055247*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2055247[Bugzilla:2055247]) 

[id="BZ-2076646"]
* Previously, uninstalling a cluster using the installation program failed to delete all resources in clusters installed on GCP if the cluster name was more than 22 characters long. In this update, uninstalling a cluster using the installation program correctly locates and deletes all GCP cluster resources in cases of long cluster names. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2076646[*BZ#2076646*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2076646[Bugzilla:2076646]) 

[id="BZ-2095323"]
* Previously, when installing a cluster on :rh-openstack-first: with multiple networks defined in the `machineNetwork` parameter, the installation program only created security group rules for the first network. With this update, the installation program creates security group rules for all networks defined in the `machineNetwork` so that users no longer need to manually edit security group rules after installation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2095323[*BZ#2095323*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2095323[Bugzilla:2095323]) 

[id="BZ-2097691"]
* Previously, when installing a cluster on vSphere using a datacenter that is embedded inside a folder, the installation program could not locate the datacenter object, causing the installation to fail. In this update, the installation program can traverse the directory that contains the datacenter object, allowing the installation to succeed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2097691[*BZ2097691*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2097691[Bugzilla:2097691]) 

[id="BZ-2098299"]
* Previously, cluster installations using Hive could fail if Hive used an older version of the install-config.yaml file. This update allows the installer to accept older versions of the install-config.yaml file provided by Hive. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2098299[*BZ#2098299*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2098299[Bugzilla:2098299]) 

[id="BZ-2102324"]
* Previously, when an installation on Google Cloud provider (GCP) failed because an invalid GCP region was specified, the resulting error message did not mention this as the cause of the failure. This update improves the error message, which now states the region is not valid. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2102324[*BZ#2102324*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102324[Bugzilla:2102324]) 

[id="BZ-2103144"]
* Previously, the installation program would incorrectly allow the `apiVIP` and `ingressVIP` parameters to use the same IPv6 address if they represented the address differently, such as listing the address in an abbreviated format. In this update, the installer validates these two parameters correctly regardless of their formatting, requiring separate IP addresses for each parameter. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2103144[*BZ#2103144*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2103144[Bugzilla:2103144]) 

[id="BZ-2103236"]
* Previously, if a cluster failed to install on Google Cloud Platform because the service account had insufficient permissions, the resulting error message did not mention this as the cause of the failure. This update improves the error message, which now instructs users to check the permissions that are assigned to the service account. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2103236[*BZ#2103236*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2103236[Bugzilla:2103236]) 

[id="BZ-2105341"]
* Previously, gathering bootstrap logs was not possible until the control plane machines were running. With this update, gathering bootstrap logs now only requires that the bootstrap machine be available. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2105341[*BZ#2105341*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2105341[Bugzilla:2105341]) 

[id="BZ-2109388"]
* Previously, installing a cluster on Amazon Web Services started and then failed when the IAM administrative user was not assigned the `s3:GetBucketPolicy` permission. This update adds this policy to checklist that the installation program uses to ensure that all of the required permissions are assigned. As a result, the installation program now stops the installation with a warning that the IAM administrative user is missing the `s3:GetBucketPolicy` permission. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2109388[*BZ#2109388*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2109388[Bugzilla:2109388]) 

[id="BZ-2118286"]
* Previously, the `kube-controller-manager` Operator was reporting `degraded` on environments without a monitoring stack presence. With this update, the `kube-controller-manager` Operator skips checking the monitoring for cues about degradation when the monitoring stack is not present. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2118286[*BZ#2118286*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2118286[Bugzilla:2118286]) 

[id="BZ-2100923"]
* Previously, the secondary scheduler deployment was not deleted after a secondary scheduler custom resource was deleted. Consequently, the Secondary Schedule Operator and Operand were not fully uninstalled. With this update, the correct owner reference is set in the secondary scheduler custom resource so that it points to the secondary scheduler deployment. As a result, secondary scheduler deployments are deleted when the secondary scheduler custom resource is deleted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2100923[*BZ#2100923*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2100923[Bugzilla:2100923]) 

[id="BZ-2034883"]
* Previously, the Machine Config Operator (MCO) `ControllerConfig` resource, which contains important certificates, was only synced if the Operator's daemon sync succeeded. By design, unready nodes during a daemon sync prevent that daemon sync from succeeding, so unready nodes were indirectly preventing the `ControllerConfig` resource, and therefore those certificates, from syncing. This resulted in eventual cluster degradation when there were unready nodes due to inability to rotate the certificates contained in the `ControllerConfig` resource. With this release, the sync of the `ControllerConfig` resource is no longer dependent on the daemon sync succeeding, so the `ControllerConfig` resource now continues to sync if the daemon sync fails. This means that unready nodes no longer prevent the `ControllerConfig` resource from syncing, so certificates continue to be updated even when there are unready nodes.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2034883[*2034883*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2034883[Bugzilla:2034883]) 

[id="BZ-1951901"]
* Previously, the wrong calculating method was used when counter master and worker nodes. With this update, the correct worker nodes are calculated when nodes have both the `master` and `worker` role. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1951901[*BZ#1951901*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=1951901[Bugzilla:1951901]) 

[id="BZ-2037329"]
* Previously, there were redundant checks for the model resulting in tab reloading which occasionally resulted in a flickering of the tab contents where they re-rendered. With this update, the redundant model check was removed, and the model is only checked once. As a result, the tab contents do not flicker and no longer re-render. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2037329[*BZ#2037329*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2037329[Bugzilla:2037329]) 

[id="BZ-2052662"]
* Previously, if issues were pending, clicking on the *Insights* link would crash the page. As a workaround, you can wait for the variable to become `initialized` before clicking the *Insights* link. As a result, the Insights page will open as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2052662[*BZ#2052662*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2052662[Bugzilla:2052662]) 

[id="BZ-2080260"]
* Previously, conflicting `react-router` routes for `ImageManifestVuln` resulted in attempts to render a details page for `ImageManifestVuln` with a `~new` name. Now, the container security plugin has been updated to remove conflicting routes and to ensure dynamic lists and details page extensions are used on the Operator details page. As a result, the console renders correct create, list, and details pages for `ImageManifestVuln`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2080260[*BZ#2080260*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2080260[Bugzilla:2080260]) 

[id="BZ-2084453"]
* Previously, there was a coding error, yaml that was not synced was occasionally displayed to users. With this update, synced yaml always displays. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2084453[*BZ#2084453*]

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2084453[Bugzilla:2084453]) 

[id="BZ-2094240"]
* Previously, when the `MachineConfigPool` resource was paused, the option to unpause said *Resume rollouts*. The wording has been updated so that it now says *Resume updates`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2094240[*BZ#2094240*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2094240[Bugzilla:2094240]) 

[id="BZ-2094502"]
* Previously, when installing an Operator that required a custom resource (CR) to be created for use, the *Create resource* button could fail to install the CR because it was pointing to the incorrect namespace. With this update, the *Create resource* button works as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2094502[*BZ#2094502*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2094502[Bugzilla:2094502]) 

[id="BZ-2096350"]
* Previously, the *Cluster update* modal was not displaying errors properly. As a result, the *Cluster update* modal did not display or explain errors when they occurred. With this update, the *Cluster update* modal was updated to correctly display errors. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2096350[*BZ#2096350*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2096350[Bugzilla:2096350]) 

[id="BZ-2098234"]
* Previously, in the administrator perspective of the web console, the link to *_Learn more about the OpenShift local update services_* on the *Default update server* pop-up window in the *Cluster Settings* page produces a 404 error. With this update, the link works as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2098234[*BZ#2098234*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2098234[Bugzilla:2098234]) 

[id="BZ-2102098"]
* Previously, when selecting the `edit` label from the action list on the OpenShift Dedicated node page, no response was elicited and a web hook error was returned. This issue has been fixed so that the error message is only returned when editing fails. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2102098[*BZ#2102098*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102098[Bugzilla:2102098]) 

[id="BZ-2039411"]
* Before this update, if the Cluster Monitoring Operator (CMO) failed to update Prometheus, the CMO did not verify whether a previous deployment was running and would report that cluster monitoring was unavailable even if one of the Prometheus pods was still running. With this update, the CMO now checks for running Prometheus pods in this situation and reports that cluster monitoring is unavailable only if no Prometheus pods are running.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2039411[*BZ#2039411*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2039411[Bugzilla:2039411]) 

[id="BZ-2043518"]
* Before this update, if Prometheus Operator failed to run or schedule Prometheus pods, the system provided no underlying reason for the failure. With this update, if Prometheus pods are not run or scheduled, the Cluster Monitoring Operator updates the `clusterOperator` monitoring status with a reason for the failure, which can be used to troubleshoot the underlying issue.   (link:https://bugzilla.redhat.com/show_bug.cgi?id=2043518[*BZ#2043518*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2043518[Bugzilla:2043518]) 

[id="BZ-2083226"]
* Before this update, Alertmanager pod startup might time out because of slow DNS resolution, and the Alertmanager pods would not start. With this release, the timeout value has been increased to seven minutes, which prevents pod startup from timing out.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2083226[*BZ#2083226*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2083226[Bugzilla:2083226]) 

[id="BZ-2084504"]
* Before this update, if you created an alert silence from the *Developer* perspective in the {product-title} web console, external labels were included that did not match the alert. Therefore, the alert would not be silenced. With this update, external labels are now excluded when you create a silence in the *Developer* perspective so that newly created silences function as expected.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2084504[*BZ#2084504*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2084504[Bugzilla:2084504]) 

[id="BZ-2093892"]
* Before this update, if you configured OpsGenie as an alert receiver, a warning would appear in the log that `api_key` and `api_key_file` are mutually exclusive and that `api_key` takes precedence. This warning appeared even if you had not defined `api_key_file`. With this update, this warning only appears in the log if you have defined both  `api_key` and `api_key_file`.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2093892[*BZ#2093892*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2093892[Bugzilla:2093892]) 

[id="BZ-2099939"]
* Previously, if you enabled an instance of Alertmanager dedicated to user-defined projects, a misconfiguration could occur in certain circumstances, and you would not be informed that the user-defined project Alertmanager config map settings did not load for either the main instance of Alertmanager or the instance dedicated to user-defined projects. With this release, if this misconfiguration occurs, the Cluster Monitoring Operator now displays a message that informs you of the issue and provides resolution steps. 
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2099939[*BZ#2099939*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2099939[Bugzilla:2099939]) 

[id="BZ-2114721"]
* Before this update the Telemeter Client (TC) only loaded new pull secrets when it was manually restarted. Therefore, if a pull secret had been changed or updated and the TC had not been restarted, the TC would fail to authenticate with the server. This update addresses the issue so that when the secret is rotated, the deployment is automatically restarted and uses the updated token to authenticate.
(link:https://bugzilla.redhat.com/show_bug.cgi?id=2114721[*BZ#2114721*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2114721[Bugzilla:2114721]) 

[id="BZ-2096413"]
* Previously, `ovn-kubernetes` did not configure the correct mac address of bond interfaces in `br-ex` bridge. As a result, a node that uses bonding for the primary Kubernetes interface fails to join the cluster. With this update, `ovn-kubernetes` configures the correct mac address of bond interfaces in `br-ex` bridge, and nodes that use bonding for the primary Kubernetes interface successfully join the cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2096413[*BZ2096413*]

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2096413[Bugzilla:2096413]) 

[id="BZ-2117524"]
* Previously, when the Ingress Operator was configured to enable the use of mTLS, the Operator would not check if CRLs were due to be updated until some other event caused it to reconcile. As a result, CRLs used for mTLS could become out of date. With this update, the Ingress Operator now automatically reconciles when any CRL expires, and CRLs will be updated at the time specified by their `nextUpdate` field. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2117524[*BZ#2117524*]

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2117524[Bugzilla:2117524]) 

[id="BZ-1977660"]
* Previously, a symlinks error message was printed out as raw data instead of formatted as an error, making it difficult to understand. This fix formats the error message properly, so that it is easily understood. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1977660[*BZ#1977660*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=1977660[Bugzilla:1977660]) 

[id="BZ-2059125"]
* Previously, on macOS arm64 architecture, the `oc` binary needed to be signed manually. As a result, the `oc` binary did not work as expected. This update implements a self-signing binary for `oc` mimicking. As a result, the `oc` binary on macOS arm64 architectures works properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2059125[*BZ#2059125*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2059125[Bugzilla:2059125]) 

[id="BZ-2095708"]
* Previously, `must-gather` was trying to collect resources that were not present on the server. Consequently, `must-gather` would print error messages. Now, before collecting resources, `must-gather` checks whether the resource exists. As a result, `must-gather` no longer prints an error when it fails to collect non-existing resources on the server. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2095708[*BZ#2095708*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2095708[Bugzilla:2095708]) 

[id="BZ-2097557"]
* Previously, Operator Lifecycle Manager (OLM) would attempt to update namespaces to apply a label, even if the label was present on the namespace. Consequently, the update requests increased the workload in API and etcd services. With this update, OLM compares existing labels against the expected labels on a namespace before issuing an update. As a result, OLM no longer attempts to make unnecessary update requests on namespaces. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2105045[*BZ#2105045*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2097557[Bugzilla:2097557]) 

[id="BZ-2105045"]
* Previously, Operator Lifecycle Manager (OLM) would attempt to update namespaces to apply a label, even if the label was present on the namespace. Consequently, the update requests increased the workload in API and etcd services. With this update, OLM compares existing labels against the expected labels on a namespace before issuing an update. As a result, OLM no longer attempts to make unnecessary update requests on namespaces. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2105045[*BZ#2105045*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2105045[Bugzilla:2105045]) 

[id="BZ-2091864"]
* With this update, you can now set the security context for the registry pod by including the `securityContext` configuration field in the pod specification. This will apply the security context for all containers in the pod. The `securityContext` field also defines the pod's privileges. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2091864[*BZ#2091864*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2091864[Bugzilla:2091864]) 

[id="BZ-1915537"]
* Previously, the `podman exec` command did not work well with nested containers. Users encountered this issue when accessing a node using the `oc debug` command and then running a container with the `toolbox` command. Because of this, users were unable to reuse toolboxes on {op-system}. This fix updates the toolbox library code to account for this behavior, so users can now reuse toolboxes on {op-system}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1915537[*BZ#1915537*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=1915537[Bugzilla:1915537]) 

[id="BZ-2048789"]
* Previously, updating to Podman 4.0 prevented users from using custom images with toolbox containers on {op-system}. This fix updates the toolbox library code to account for the new Podman behavior, so users can now use custom images with toolbox on {op-system} as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2048789[*BZ#2048789*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2048789[Bugzilla:2048789]) 

[id="BZ-2093040"]
* Previously, updating to Podman 4.0 prevented users from running the `toolbox` command on {op-system}. This fix updates the toolbox library code to account for the new Podman behavior, so users can now run `toolbox` on {op-system} as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2093040[*BZ#2093040*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2093040[Bugzilla:2093040]) 

[id="BZ-2022328"]
* Previously, {product-title} detachec a volume when a Container Storage Interface (CSI) driver was not able to unmount the volume from a node. Detaching a volume without unmount is not allowed by CSI specifications and drivers could enter `undocumented` state. With this update, CSI drivers are detached before unmount only on unhealthy nodes preventing the `undocumented` state. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2049306[*BZ#2049306*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2022328[Bugzilla:2022328]) 

[id="BZ-2057637"]
* Previously, there were missing annotations on the Manila CSI Driver Operator's VolumeSnapshotClass. Consequently, the Manila CSI snapshotter could not locate secrets, and could not create snapshots with the default VolumeSnapshotClass. This update fixes the issue so that secret names and namespace are included in the default VolumeSnapshotClass. As a result, users can now create snapshots in the Manila CSI Driver Operator using the default VolumeSnapshotClass. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2057637[*BZ#2057637*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2057637[Bugzilla:2057637]) 

[id="BZ-2082773"]
* Previously, checks for generic ephemeral volumes failed. With this update, checks for expandable volumes now include generic ephemeral volumes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2082773[*BZ#2082773*]

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2082773[Bugzilla:2082773]) 

[id="BZ-2108473"]
* Previously, if more than one secret was present for vSphere, the vSphere CSI Operator randomly picked a secret and sometimes caused the Operator to restart. With this update, a warning appears when there is more than one secret on the vCenter CSI Operator. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2108473[*BZ#2108473*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2108473[Bugzilla:2108473]) 

[id="BZ-2107261"]
* Previously, restarting the Windows Machine Config Operator (WMCO) in a cluster with running Windows nodes caused the Windows exporter endpoint to be removed. Because of this, each Windows node could not report any metrics data. With this update, the endpoint is retained when the WMCO is started. As a result, metrics data is reported properly after restarting WMCO. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2107261[*BZ#2107261*])

(link:https://bugzilla.redhat.com/show_bug.cgi?id=2107261[Bugzilla:2107261]) 
