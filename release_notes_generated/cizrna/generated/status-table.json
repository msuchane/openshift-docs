{"products":"OpenShift Container Platform, RHEL Planning","release":"9.2.0, 8.8.0","overall_progress":{"all":87,"complete":2,"complete_pct":2.2988505747126435,"warnings":0,"warnings_pct":0.0,"incomplete":85,"incomplete_pct":97.70114942528735},"tickets_with_checks":[[{"id":"Bugzilla:2111842","summary":"vSphere test failure: [Serial] [sig-auth][Feature:OAuthServer] [RequestHeaders] [IdP] test RequestHeaders IdP [Suite:openshift/conformance/serial]","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Cluster Authentication Operator state was set to `progressing = false` after receiving a `workloadIsBeingUpdatedTooLong` error. At the same time, `degraded = false` was kept for the time of the `inertia` defined. Consequently, the shortened amount of progressing and increased time of degraded would create a situation where `progressing = false` and `degraded = false` were set prematurely. This caused inconsistent OpenShift CI tests because a healthy state was assumed, which was incorrect. This issue has been fixed by removing the `progressing = false` setting after the `workloadIsBeingUpdatedTooLong` error is returned. Now, because there is no `progressing = false` state, OpenShift CI tests are more consistent. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2111842#h11[*BZ2111842*]).","docs_contact":"Missing docs contact","doc_text_status":"WIP","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2111842","assignee":"kostrows@redhat.com","components":["apiserver-auth"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: ?","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2111842:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":{"Error":"RN not approved."},"title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2099991","summary":"pass the \"--quiet\" option via the buildconfig for s2i","description":null,"doc_type":"Bug Fix","doc_text":"* By default, Buildah prints steps to the log file, including the contents of environment variables, which might include xref:../cicd/builds/creating-build-inputs.adoc#builds-input-secrets-configmaps_creating-build-inputs[build input secrets]. Although you can use the `--quiet` build argument to suppress printing of those environment variables, this argument isn't available if you use the source-to-image (S2I) build strategy. The current release fixes this issue. To suppress printing of environment variables, set the `BUILDAH_QUIET` environment variable in your build configuration:\r\n+\r\n[source,yaml]\r\n----\r\nsourceStrategy:\r\n...\r\n  env:\r\n    - name: \"BUILDAH_QUIET\"\r\n      value: \"true\"\r\n----","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2099991","assignee":"cdaley@redhat.com","components":["Build"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2099991:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:1997396","summary":"No alerts have triggered for CPU and Memory limit with Cluster Autoscaler","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the cluster autoscaler metrics for cluster CPU and memory usage would never reach, or exceed, the limits set by the `ClusterAutoscaler` resource. As a result, no alerts were fired when the cluster autoscaler could not scale due to resource limitations. With this release, a new metric called `cluster_autoscaler_skipped_scale_events_count` is added to the cluster autoscaler to more accurately detect when resource limits are reached or exceeded. Alerts will now fire when the cluster autoscaler is unable to scale the cluster up because it has reached the cluster resource limits. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1997396[*BZ#1997396*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=1997396","assignee":"mimccune@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 1997396:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2001027","summary":"ClusterAutoscaler with balanceSimilarNodeGroups does not scale even across MachineSet","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the cluster autoscaler did not respect the AWS, IBM Cloud, and Alibaba Cloud topology labels for the CSI drivers when using the Cluster API provider. As a result, nodes with the topology label were not processed properly by the autoscaler when attempting to balance nodes during a scale-out event. With this release, the autoscaler's custom processors are updated so that it respects this label. The autoscaler can now balance similar node groups that are labelled by the the AWS, IBM Cloud, or Alibaba CSI labels. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001027[*BZ#2001027*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2001027","assignee":"mimccune@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2001027:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2056387","summary":"[IPI on Alibabacloud][RHEL scaleup] new RHEL worker were not added into the backend of Ingress SLB automatically","description":null,"doc_type":"Known Issue","doc_text":"Unclear status, see https://bugzilla.redhat.com/show_bug.cgi?id=2056387#c41\r\n\r\nProposed doc text:\r\n\r\n* When scaling up an Alibaba Cloud cluster with {op-system-base} compute nodes, the new nodes show as `Ready`, but the Ingress pods do not transition to `Running` on these nodes. As a result, the scale-up operation does not succeed. As a workaround, you can perform a scale-up operation with {op-system} compute nodes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2056387[*BZ#2056387*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2056387","assignee":"brlu@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","needinfo: ?","needinfo: ?","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2056387:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":"Ok"}],[{"id":"Bugzilla:2060068","summary":"machine-api-provider-aws creates EC2 instances with the default security group when no matching security group is found","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Machine API provider for AWS did not verify that the security group defined in the machine specification exists. Instead of returning an error in this case, it used a default security group, which should not be used for {product-title} machines, and successfully created a machine without informing the user that the default group was used. With this release, the Machine API returns an error when users set either incorrect or empty security group names in the machine specification. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2060068[*BZ#2060068*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2060068","assignee":"mfedosin@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: -","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2060068:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2062579","summary":"[IBMCloud] Provide invalid profile machine stuck in \"Provisioning\" phase","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when creating a new `Machine` resource using a machine profile that does not exist in IBM Cloud, the machines became stuck in the `Provisioning` phase. With this release, validation is added to the IBM Cloud Machine API provider to ensure that a machine profile exists, an machines with an invalid machine profile are rejected by the Machine API. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2062579[*BZ#2062579*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2062579","assignee":"cschaefe@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2062579:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2085390","summary":"machine-controller is case sensitive which can lead to false/positive errors","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Machine API provider Azure did not treated user-provided values for instance types as case sensitive. This led to false-positive errors when instance types were correct but did not match the case. With this release, instance types are converted to the lower case so that users get correct results without false-positive errors for mismatched case. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2085390[*BZ#2085390*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2085390","assignee":"mfedosin@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","needinfo: ?","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2085390:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2087981","summary":"PowerOnVM_Task is deprecated use PowerOnMultiVM_Task for DRS ClusterRecommendation","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Machine API vSphere machine controller set the `PowerOn` flag when cloning a VM. This created a `PowerOn` task that the machine controller was not aware of. If that `PowerOn` task failed, machines were stuck in the `Provisioned` phase but never powered on. With this release, the cloning sequence is altered to avoid the issue. Additionally, the machine controller now retries powering on the VM in case of failure and reports failures properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2087981[*BZ#2087981*], link:https://issues.redhat.com/browse/OCPBUGS-954[*OCPBUGS-954*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2087981","assignee":"dmoiseev@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: -","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2087981:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2098054","summary":"The control plane should tag AWS security groups at creation","description":null,"doc_type":"Enhancement","doc_text":"* With this release, AWS security groups are tagged immediately instead of after creation. This means that fewer requests are sent to AWS and the required user privileges are lowered. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2098054[*BZ#2098054*], link:https://issues.redhat.com/browse/OCPBUGS-3094[*OCPBUGS-3094*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2098054","assignee":"mfedosin@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2098054:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2101736","summary":"Finalizers can't be removed for machines","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, machines created in early versions of {product-title} with invalid configurations could not be deleted. With this release, the webhooks that prevent the creation of machines with invalid configurations no longer prevent the deletion of existing invalid machines. Users can now successfully remove these machines from their cluster by manually removing the finalizers on these machines. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2101736[*BZ#2101736*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2101736","assignee":"mfedosin@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: -","requires_doc_text: +","blocker: ?"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2101736:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2102383","summary":"Kube controllers crash when nodes are shut off in OpenStack","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, a bug in the {rh-openstack} legacy cloud provider resulted in a crash if certain {rh-openstack} operations were attempted after authentication had failed. For example, shutting down a server causes the Kubernetes controller manager to fetch server information from {rh-openstack}, which triggered this bug. As a result, if initial cloud authentication failed or was configured incorrectly, shutting down a server caused the Kubernetes controller manager to crash. With this release, the {rh-openstack} legacy cloud provider is updated to not attempt any {rh-openstack} API calls if it has not previously authenticated successfully. Now, shutting down a server with invalid cloud credentials no longer causes Kubernetes controller manager to crash.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102383[*BZ#2102383*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2102383","assignee":"mbooth@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["qe_test_coverage: ?","requires_doc_text: +","blocker: ?"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2102383:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2104373","summary":"[AWS] CCM cannot work on Commercial Cloud Services (C2S) Top Secret Region","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Cloud Controller Manager Operator did not check the `cloud-config` configuration file for AWS clusters. As a result, it was not possible to pass additional settings to the AWS cloud controller manager component by using the configuration file. With this release, the Cloud Controller Manager Operator checks the infrastructure resource and parses references to the `cloud-config` configuration file so that users can configure additional settings. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2104373[*BZ#2104373*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2104373","assignee":"dmoiseev@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2104373:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2106733","summary":"Machine Controller stuck with Terminated Instances while Provisioning on AWS","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, there was no check for nil values in the annotations of a machine object before attempting to access the object. This situation was rare, but caused the machine controller to panic when reconciling the machine. With this release, nil values are checked and the machine controller is able to reconcile machines without annotations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2106733[*BZ#2106733*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2106733","assignee":"rmanak@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2106733:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2108647","summary":"[azure] Standard_D2s_v3 as worker failed by “accelerated networking not supported on instance type”","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when Azure added new instance types and enabled accelerated networking support on instance types that previously did not have it, the list of Azure instances in the machine controller became outdated. As a result, the machine controller could not create machines with instance types that did not previously support accelerated networking, even if they support this feature on Azure. With this release, the required instance type information is retrieved from Azure API before the machine is created to keep it up to date and the machine controller is able to create machines with new and updated instance types. This fix also applies to any instance types that are added in the future. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2108647[*BZ#2108647*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2108647","assignee":"rmanak@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2108647:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2111467","summary":"Node internal DNS address is not set for machine","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when the Machine API provider failed to fetch the machine IP address, it would not set the internal DNS name and the machine certificate signing requests were not automatically approved. With this release, the Power VS machine provider is updated to set the server name as the internal DNS name even when it fails to fetch the IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2111467[*BZ#2111467*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2111467","assignee":"kabhat@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2111467:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2111474","summary":"Fetch internal IPs of vms from dhcp server","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, Power VS cloud providers were not capable of fetching the machine IP address from DHCP server. Changing the IP address did not update the node, which caused some inconsistencies, such as pending certificate signing requests. With this release, the Power VS cloud provider is updated to fetch the machine IP address from the DHCP server so that the IP addresses for the nodes are consistent with the machine IP address. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2111474[*BZ#2111474*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2111474","assignee":"kabhat@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2111474:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2115090","summary":"[IBMCloud][OVN] Install fails, worker machines stuck in Provisioned \"has no node yet\"","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, short DHCP lease times in combination with `NetworkManager` not being run as a daemon or in continuous mode caused machines to become stuck during their ignition configuration never become nodes in the cluster. With this release, extra checks are added so that if a machine becomes stuck in this state it is deleted and recreated  automatically. Machines that are affected by this network condition can become nodes after a reboot from the Machine API controller. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2115090[*BZ#2115090*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2115090","assignee":"cschaefe@redhat.com","components":["Cloud Compute"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2115090:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2051443","summary":"OSLAT runner uses both sibling threads causing latency spikes","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the `oslat` runner configured `oslat` to use all available CPUs, which caused false spikes. With this update, the `oslat` runner reserves one CPU for the control thread. As a result, false spikes no longer occur. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2051443[*BZ#2051443*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"VERIFIED","is_open":true,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2051443","assignee":"titzhak@redhat.com","components":["CNF Platform Validation"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: ?"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2051443:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2060726","summary":"Compliance operator does not generate alert notification for non-control namespace","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Compliance Operator hard-coded notifications to the default namespace. As a result, notifications from the Operator would not appear if the Operator was installed in a different namespace. This issue is fixed in this release.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2060726[*BZ#2060726*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2060726","assignee":"mrogers@redhat.com","components":["Compliance Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2060726:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2094382","summary":"Auto remediation does not work for rules rhcos4-high-master-sysctl-kernel-yama-ptrace-scope and rhcos4-sysctl-kernel-core-pattern","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, applying automatic remediation for the `rhcos4-high-master-sysctl-kernel-yama-ptrace-scope` and `rhcos4-sysctl-kernel-core-pattern` rules resulted in subsequent failures of those rules in scan results, even though they were remediated. The issue is fixed in this release.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2094382[*BZ#2094382*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2094382","assignee":"wenshen@redhat.com","components":["Compliance Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2094382:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2098581","summary":"APIRemovedInNextEUSReleaseInUse alert fired for openshift-compliance cronjobs","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Compliance Operator used an old version of the Operator SDK, which is a dependency for building Operators. This caused alerts about deprecated Kubernetes functionality used by the Operator SDK. With this release, the Compliance Operator is upgraded to version 0.1.55, which includes an updated version of the Operator SDK.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2098581[*BZ#2098581*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2098581","assignee":"mrogers@redhat.com","components":["Compliance Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2098581:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2102511","summary":"[OSD] mcp puase status stuck at true issue as Compliance Operator failed to check if kubeletconfig custom-kubelet is subset of rendered MC 99-worker-generated-kubelet","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Compliance Operator held machine configurations in a stuck state because it could not determine the relationship between machine configurations and kubelet configurations due to incorrect assumptions about machine configuration names. With this release, the Compliance Operator is able to determine if a kubelet configuration is a subset of a machine configuration.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2102511[*BZ#2102511*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2102511","assignee":"wenshen@redhat.com","components":["Compliance Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2102511:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2117268","summary":"ocp4-pci-dss-api-checks-pod in CrashLoopBackoff state due to ignition spec.config not in MC","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Compliance Operator failed to fetch API resources when parsing machine configurations without ignition specifications. This caused the `api-check-pods` check to crash loop. With this release, the Compliance Operator is updated to gracefully handle machine config pools without ignition specifications.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2117268[*BZ#2117268*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2117268","assignee":"jhrozek@redhat.com","components":["Compliance Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2117268:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2071792","summary":"Non-kubeadmin user will not have access to openshift-config ns to pull secret/CM for adding private HCR in a namespace","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the `openshift-config` namespace was hardcoded for the `HelmChartRepository` custom resource, which was the same namespace for the `ProjectHelmChartRepository` custom resource. This prevent users from adding private `ProjectHelmChartRepository` custom resources in their desired namespace. Consquently, users were unable to access secrets and configmaps in the `openshift-config` namespace. This update fixes the project Helm chart repository custom resource definition with a namespace field that can read the secret and configmaps from a namespace of choice by a user with the correct permissions. Additionally, the user can add secrets and configmaps to the accessible namespace, and they can add private Helm cart repositories in the namespace used the creation resources. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2071792[*BZ#2071792*])","docs_contact":"ssiddhar@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2071792","assignee":"kmamgain@redhat.com","components":["Dev Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2071792:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2089221","summary":"Could not de-select a Git Secret in add and edit forms","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the users could not deselect a Git secret in add and edit forms. As a result, the resources had to be recreated. This fix resolves the issue by adding the option to choose `No Secret` in the select secret option list. As a result, the users can easily select, deselect, or detach any attached secrets.  (link:https://bugzilla.redhat.com/show_bug.cgi?id=2089221[*BZ#2089221*])","docs_contact":"ssiddhar@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2089221","assignee":"akundu@redhat.com","components":["Dev Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2089221:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2101393","summary":"NodeHasIntegrityFailure alert doesn't set namespace label","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, alerts issued by the File Integrity Operator did not set a namespace. This made it difficult to understand where the alert was coming from, or what component was responsible for issuing it. With this release, the Operator includes the namespace it was installed into in the alert, making it easier to narrow down what component needs attention.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2101393[*2101393*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2101393","assignee":"jhrozek@redhat.com","components":["File Integrity Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2101393:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2104897","summary":"Failed to create Fileintegrity object in a namespace without openshift prefix","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the File Integrity Operator deployed templates using the `openshift-file-integrity` namespace in the permissions for the Operator. When the Operator attempted to create objects in the namespace, to would fail due to permission issues. With this release, the deployment resources used by OLM are updated to use the correct namespace, fixing the permission issues so that users can install and use the operator in a non-default namespaces.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2104897[*BZ#2104897*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2104897","assignee":"mrogers@redhat.com","components":["File Integrity Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2104897:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2108475","summary":"Unable to upgrade file integrity operator using catalog source","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the File Integrity Operator daemon used the `ClusterRoles` parameter instead of the `Roles` parameter for a recent permission change. As a result, OLM could not upgrade the Operator. With this release, the Operator daemon reverts to using the `Roles` parameter and upgrades from older versions to version 0.1.29 are successful.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2108475[*BZ#2108475*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2108475","assignee":"mrogers@redhat.com","components":["File Integrity Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: ?"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2108475:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2109153","summary":"pods for daemonset failed to create after file integrity operator upgrade from v0.1.24 to v0.1.29","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, service account ownership for the File Integrity Operator regressed due to underlying OLM updates, and updates from 0.1.24 to 0.1.29 were broken. With this update, the Operator should default to upgrading to 0.1.30. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2109153[*BZ#2109153*]","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2109153","assignee":"mrogers@redhat.com","components":["File Integrity Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2109153:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2112394","summary":"NodeHasIntegrityFailure alert doesn't set namespace label after upgrade file-integrity-operator from v0.1.24>v0.1.30","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the File Integrity Operator did not properly handle modifying alerts during an upgrade. As a result, alerts did not include the namespace in which the Operator was installed. With this release, the Operator includes the namespace it was installed into in the alert, making it easier to narrow down what component needs attention.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2112394[*2112394*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2112394","assignee":"mrogers@redhat.com","components":["File Integrity Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2112394:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2115821","summary":"FIO Target Down Alert firing","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, underlying dependencies of the File Integrity Operator changed how alerts and notifications were handled, and the Operator didn't send metrics as a result. With this release the Operator ensures that the metrics endpoint is correct and reachable on startup.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2115821[*2115821*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2115821","assignee":"mrogers@redhat.com","components":["File Integrity Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2115821:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2055620","summary":"ImageStreamChange triggers using annotations does not work","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the image trigger controller did not have permissions to change objects. Consequently, image trigger annotations did not work on some resources. This update creates cluster role binding that provides the controller the required permissions to update objects according to annotations. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055620([*BZ#2055620*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2055620","assignee":"obulatov@redhat.com","components":["Image Registry"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: -","qe_test_coverage: ?","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2055620:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2093440","summary":"[sig-arch][Early] Managed cluster should start all core operators  - NodeCADaemonControllerDegraded: failed to update object","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Image Registry Operator did not have a `progressing` condition for the `node-ca` daemon set and used `generation` from an incorrect object. Consequently, the `node-ca` daemon set could be marked as `degraded` while the Operator was still running. This update adds the `progressing` condition, which indicates that the installation is not complete. As a result, the Image Registry Operator successfully installs the `node-ca` daemon set and the installer waits until it is fully deployed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2093440[(*BZ#2093440*)]","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2093440","assignee":"obulatov@redhat.com","components":["Image Registry"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2093440:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:1944365","summary":"openstack: missing validation for apiVIP and ingressVIP","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, users could manually set the API and Ingress virtual IP addresses to values that conflicted with the allocation pool of the DHCP server when installing a cluster on OpenStack. This could cause the DHCP server to assign one of the VIP addresses to a new machine, which would fail to start. In this update, the installation program validates the user-provided VIP addresses to ensure that they do not conflict with any DHCP pools. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1944365[*BZ1944365*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=1944365","assignee":"pprinett@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: -","qe_test_coverage: ?","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 1944365:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2055247","summary":"[Azure] Fail to create master nodes with dcasv5 /dcadsv5 -series Confidential Virtual Machine","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, installing a cluster on Microsoft Azure failed when the Azure DCasv5-series or DCadsv5-series of confidential VMs were specified as control plane nodes. With this update, the installation program now stops the installation with an error, which states that confidential VMs are not yet supported. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055247[*BZ#2055247*])","docs_contact":"mpytlak@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2055247","assignee":"jhixson@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","needinfo: -","needinfo: -","needinfo: -","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2055247:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2076646","summary":"openshift-install destroy unable to delete PVC disks in GCP if cluster identifier is longer than 22 characters","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, uninstalling a cluster using the installation program failed to delete all resources in clusters installed on GCP if the cluster name was more than 22 characters long. In this update, uninstalling a cluster using the installation program correctly locates and deletes all GCP cluster resources in cases of long cluster names. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2076646[*BZ#2076646*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2076646","assignee":"bbarbach@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["needinfo: ?","needinfo: ?","needinfo: ?","needinfo: ?","needinfo: ?","needinfo: ?","needinfo: ?","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2076646:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2095323","summary":"Openshift on OpenStack does not honor machineNetwork setting with multiple networks","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when installing a cluster on :rh-openstack-first: with multiple networks defined in the `machineNetwork` parameter, the installation program only created security group rules for the first network. With this update, the installation program creates security group rules for all networks defined in the `machineNetwork` so that users no longer need to manually edit security group rules after installation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2095323[*BZ#2095323*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2095323","assignee":"m.andre@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["qe_test_coverage: ?","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2095323:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2097691","summary":"[vsphere] failed to create cluster if datacenter is embedded in a Folder","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when installing a cluster on vSphere using a datacenter that is embedded inside a folder, the installation program could not locate the datacenter object, causing the installation to fail. In this update, the installation program can traverse the directory that contains the datacenter object, allowing the installation to succeed. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2097691[*BZ2097691*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2097691","assignee":"ocp-installer@bot.bugzilla.redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["needinfo: -","needinfo: -","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2097691:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2098299","summary":"install-config: Strict unmarshalling conflicts with new fields","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, cluster installations using Hive could fail if Hive used an older version of the install-config.yaml file. This update allows the installer to accept older versions of the install-config.yaml file provided by Hive. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2098299[*BZ#2098299*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2098299","assignee":"anarayan@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2098299:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2102324","summary":"GCP: Panic when unknown region AND machinesets specified in install config","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when an installation on Google Cloud provider (GCP) failed because an invalid GCP region was specified, the resulting error message did not mention this as the cause of the failure. This update improves the error message, which now states the region is not valid. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2102324[*BZ#2102324*])","docs_contact":"mpytlak@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2102324","assignee":"anarayan@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2102324:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2103144","summary":"[IPv6] apiVIP and ingressVIP non-equality validation doesn't account for synonyms","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the installation program would incorrectly allow the `apiVIP` and `ingressVIP` parameters to use the same IPv6 address if they represented the address differently, such as listing the address in an abbreviated format. In this update, the installer validates these two parameters correctly regardless of their formatting, requiring separate IP addresses for each parameter. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2103144[*BZ#2103144*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2103144","assignee":"pprinett@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["qe_test_coverage: ?","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2103144:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2103236","summary":"GCP: Error message for insufficient permissions needs to be improved","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, if a cluster failed to install on Google Cloud Platform because the service account had insufficient permissions, the resulting error message did not mention this as the cause of the failure. This update improves the error message, which now instructs users to check the permissions that are assigned to the service account. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2103236[*BZ#2103236*])","docs_contact":"mpytlak@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2103236","assignee":"anarayan@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2103236:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2105341","summary":"Bootstrap Gather Fails when cluster.tfvars.json is not available in Azure","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, gathering bootstrap logs was not possible until the control plane machines were running. With this update, gathering bootstrap logs now only requires that the bootstrap machine be available. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2105341[*BZ#2105341*])","docs_contact":"mpytlak@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2105341","assignee":"bbarbach@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2105341:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2109388","summary":"[AWS] s3 GetBucketPolicy permission is missing in installer validation","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, installing a cluster on Amazon Web Services started and then failed when the IAM administrative user was not assigned the `s3:GetBucketPolicy` permission. This update adds this policy to checklist that the installation program uses to ensure that all of the required permissions are assigned. As a result, the installation program now stops the installation with a warning that the IAM administrative user is missing the `s3:GetBucketPolicy` permission. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2109388[*BZ#2109388*])","docs_contact":"mpytlak@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2109388","assignee":"bbarbach@redhat.com","components":["Installer"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: ?"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2109388:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2001409","summary":"All critical alerts should have links to a runbook","description":null,"doc_type":"Enhancement","doc_text":"Feature:\r\nkube-controller-manager alerts (KubeControllerManagerDown, PodDisruptionBudgetAtLimit, PodDisruptionBudgetLimit, GarbageCollectorSyncFailed) now have links to github runbooks.\r\n\r\nReason: \r\nThe runbooks help with understanding and debugging these alerts.\r\n\r\n* With this update, `kube-controller-manager` alerts (`KubeControllerManagerDown`, `PodDisruptionBudgetAtLimit`, `PodDisruptionBudgetLimit`, and `GarbageCollectorSyncFailed`) have links to Github  runbooks. The runbooks help users to understand debug these alerts. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2001409[*BZ#2001409*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2001409","assignee":"fkrepins@redhat.com","components":["kube-controller-manager"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","needinfo: ?","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2001409:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2118286","summary":"KCMO should not be dependent on monitoring stack","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the `kube-controller-manager` Operator was reporting `degraded` on environments without a monitoring stack presence. With this update, the `kube-controller-manager` Operator skips checking the monitoring for cues about degradation when the monitoring stack is not present. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2118286[*BZ#2118286*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2118286","assignee":"fkrepins@redhat.com","components":["kube-controller-manager"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2118286:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2100923","summary":"[SSO] Deleting secondary scheduler CR does not delete the corresponding deployment","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the secondary scheduler deployment was not deleted after a secondary scheduler custom resource was deleted. Consequently, the Secondary Schedule Operator and Operand were not fully uninstalled. With this update, the correct owner reference is set in the secondary scheduler custom resource so that it points to the secondary scheduler deployment. As a result, secondary scheduler deployments are deleted when the secondary scheduler custom resource is deleted. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2100923[*BZ#2100923*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2100923","assignee":"lseveroa@redhat.com","components":["kube-scheduler"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2100923:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2034883","summary":"MCO does not sync kubeAPIServerServingCAData to controllerconfig if there are not ready nodes","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the Machine Config Operator (MCO) `ControllerConfig` resource, which contains important certificates, was only synced if the Operator's daemon sync succeeded. By design, unready nodes during a daemon sync prevent that daemon sync from succeeding, so unready nodes were indirectly preventing the `ControllerConfig` resource, and therefore those certificates, from syncing. This resulted in eventual cluster degradation when there were unready nodes due to inability to rotate the certificates contained in the `ControllerConfig` resource. With this release, the sync of the `ControllerConfig` resource is no longer dependent on the daemon sync succeeding, so the `ControllerConfig` resource now continues to sync if the daemon sync fails. This means that unready nodes no longer prevent the `ControllerConfig` resource from syncing, so certificates continue to be updated even when there are unready nodes.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2034883[*2034883*])","docs_contact":"jrouth@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2034883","assignee":"jkyros@redhat.com","components":["Machine Config Operator"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2034883:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:1951901","summary":"incorrect Worker nodes number calculated when nodes have both master and worker role","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the wrong calculating method was used when counter master and worker nodes. With this update, the correct worker nodes are calculated when nodes have both the `master` and `worker` role. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1951901[*BZ#1951901*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=1951901","assignee":"yapei@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: -","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 1951901:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2037329","summary":"[UI] MultiClusterHub details after it's creation starts flickers, disappears and appears back (happened twice)","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, there were redundant checks for the model resulting in tab reloading which occasionally resulted in a flickering of the tab contents where they re-rendered. With this update, the redundant model check was removed, and the model is only checked once. As a result, the tab contents do not flicker and no longer re-render. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2037329[*BZ#2037329*])","docs_contact":"opayne@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2037329","assignee":"rhamilto@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2037329:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2052662","summary":"Opening Insights popup crashes the page","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, if issues were pending, clicking on the *Insights* link would crash the page. As a workaround, you can wait for the variable to become `initialized` before clicking the *Insights* link. As a result, the Insights page will open as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2052662[*BZ#2052662*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2052662","assignee":"pakratoc@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2052662:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2080260","summary":"404 not found when create Image Manifest Vulnerability on Operator \"Container Security\"","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, conflicting `react-router` routes for `ImageManifestVuln` resulted in attempts to render a details page for `ImageManifestVuln` with a `~new` name. Now, the container security plugin has been updated to remove conflicting routes and to ensure dynamic lists and details page extensions are used on the Operator details page. As a result, the console renders correct create, list, and details pages for `ImageManifestVuln`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2080260[*BZ#2080260*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2080260","assignee":"jonjacks@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2080260:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2084453","summary":"Edit PodDisruptionBudget page sometimes takes user to not synced YAML view","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, there was a coding error, yaml that was not synced was occasionally displayed to users. With this update, synced yaml always displays. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2084453[*BZ#2084453*]","docs_contact":"opayne@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2084453","assignee":"cajieh@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2084453:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2094240","summary":"MachineConfigPool details page should use consistent word for resume updating","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when the `MachineConfigPool` resource was paused, the option to unpause said *Resume rollouts*. The wording has been updated so that it now says *Resume updates`. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2094240[*BZ#2094240*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2094240","assignee":"jhadvig@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2094240:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2094502","summary":"Creating an MCH instance does not work via blue button","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when installing an Operator that required a custom resource (CR) to be created for use, the *Create resource* button could fail to install the CR because it was pointing to the incorrect namespace. With this update, the *Create resource* button works as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2094502[*BZ#2094502*])","docs_contact":"opayne@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2094502","assignee":"sgoodwin@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2094502:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2096350","summary":"Web console doesn't display webhook errors for upgrades","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the *Cluster update* modal was not displaying errors properly. As a result, the *Cluster update* modal did not display or explain errors when they occurred. With this update, the *Cluster update* modal was updated to correctly display errors. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2096350[*BZ#2096350*])","docs_contact":"opayne@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"urgent","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2096350","assignee":"rhamilto@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: ?"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2096350:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2098234","summary":"Local Update Server link 404","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, in the administrator perspective of the web console, the link to *_Learn more about the OpenShift local update services_* on the *Default update server* pop-up window in the *Cluster Settings* page produces a 404 error. With this update, the link works as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2098234[*BZ#2098234*])","docs_contact":"opayne@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2098234","assignee":"rhamilto@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2098234:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2102098","summary":"[OSD] There is no error message shown on node label edit modal","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when selecting the `edit` label from the action list on the OpenShift Dedicated node page, no response was elicited and a web hook error was returned. This issue has been fixed so that the error message is only returned when editing fails. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2102098[*BZ#2102098*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2102098","assignee":"jhadvig@redhat.com","components":["Management Console"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2102098:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2039411","summary":"Monitoring operator reports unavailable=true while one Prometheus pod is ready","description":null,"doc_type":"Bug Fix","doc_text":"* Before this update, if the Cluster Monitoring Operator (CMO) failed to update Prometheus, the CMO did not verify whether a previous deployment was running and would report that cluster monitoring was unavailable even if one of the Prometheus pods was still running. With this update, the CMO now checks for running Prometheus pods in this situation and reports that cluster monitoring is unavailable only if no Prometheus pods are running.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2039411[*BZ#2039411*])","docs_contact":"bburt@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2039411","assignee":"sthaha@redhat.com","components":["Monitoring"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2039411:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2043518","summary":"Better message in the CMO degraded/unavailable conditions when pods can't be scheduled","description":null,"doc_type":"Bug Fix","doc_text":"* Before this update, if Prometheus Operator failed to run or schedule Prometheus pods, the system provided no underlying reason for the failure. With this update, if Prometheus pods are not run or scheduled, the Cluster Monitoring Operator updates the `clusterOperator` monitoring status with a reason for the failure, which can be used to troubleshoot the underlying issue.   (link:https://bugzilla.redhat.com/show_bug.cgi?id=2043518[*BZ#2043518*])","docs_contact":"bburt@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2043518","assignee":"sthaha@redhat.com","components":["Monitoring"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2043518:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2083226","summary":"alertmanager-main pods failing to start due to startupprobe timeout","description":null,"doc_type":"Bug Fix","doc_text":"* Before this update, Alertmanager pod startup might time out because of slow DNS resolution, and the Alertmanager pods would not start. With this release, the timeout value has been increased to seven minutes, which prevents pod startup from timing out.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2083226[*BZ#2083226*])","docs_contact":"bburt@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2083226","assignee":"sthaha@redhat.com","components":["Monitoring"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2083226:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2084504","summary":"can not silent platform alert from developer console","description":null,"doc_type":"Bug Fix","doc_text":"* Before this update, if you created an alert silence from the *Developer* perspective in the {product-title} web console, external labels were included that did not match the alert. Therefore, the alert would not be silenced. With this update, external labels are now excluded when you create a silence in the *Developer* perspective so that newly created silences function as expected.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2084504[*BZ#2084504*])","docs_contact":"bburt@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2084504","assignee":"gbernal@redhat.com","components":["Monitoring"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2084504:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2093892","summary":"no api_key_file field in AlertmanagerConfig, but error message complains it","description":null,"doc_type":"Bug Fix","doc_text":"* Before this update, if you configured OpsGenie as an alert receiver, a warning would appear in the log that `api_key` and `api_key_file` are mutually exclusive and that `api_key` takes precedence. This warning appeared even if you had not defined `api_key_file`. With this update, this warning only appears in the log if you have defined both  `api_key` and `api_key_file`.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2093892[*BZ#2093892*])","docs_contact":"bburt@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2093892","assignee":"janantha@redhat.com","components":["Monitoring"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2093892:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2099939","summary":"enabled UWM alertmanager only, user project AlertmanagerConfig is not loaded to UWM alertmanager or platform alertmanager","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, if you enabled an instance of Alertmanager dedicated to user-defined projects, a misconfiguration could occur in certain circumstances, and you would not be informed that the user-defined project Alertmanager config map settings did not load for either the main instance of Alertmanager or the instance dedicated to user-defined projects. With this release, if this misconfiguration occurs, the Cluster Monitoring Operator now displays a message that informs you of the issue and provides resolution steps. \r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2099939[*BZ#2099939*])","docs_contact":"bburt@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2099939","assignee":"jmarcal@redhat.com","components":["Monitoring"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2099939:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2114721","summary":"telemeter-client pod does not use the updated pull secret when it is changed","description":null,"doc_type":"Bug Fix","doc_text":"* Before this update the Telemeter Client (TC) only loaded new pull secrets when it was manually restarted. Therefore, if a pull secret had been changed or updated and the TC had not been restarted, the TC would fail to authenticate with the server. This update addresses the issue so that when the secret is rotated, the deployment is automatically restarted and uses the updated token to authenticate.\r\n(link:https://bugzilla.redhat.com/show_bug.cgi?id=2114721[*BZ#2114721*])","docs_contact":"bburt@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2114721","assignee":"jmarcal@redhat.com","components":["Monitoring"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2114721:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2096413","summary":"br-ex not created due to default bond interface having a different mac address than expected","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, `ovn-kubernetes` did not configure the correct mac address of bond interfaces in `br-ex` bridge. As a result, a node that uses bonding for the primary Kubernetes interface fails to join the cluster. With this update, `ovn-kubernetes` configures the correct mac address of bond interfaces in `br-ex` bridge, and nodes that use bonding for the primary Kubernetes interface successfully join the cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2096413[*BZ2096413*]","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"urgent","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2096413","assignee":"jcaamano@redhat.com","components":["Networking"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2096413:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2117524","summary":"openshift-ingress-operator with mTLS does not download CRL","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, when the Ingress Operator was configured to enable the use of mTLS, the Operator would not check if CRLs were due to be updated until some other event caused it to reconcile. As a result, CRLs used for mTLS could become out of date. With this update, the Ingress Operator now automatically reconciles when any CRL expires, and CRLs will be updated at the time specified by their `nextUpdate` field. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2117524[*BZ#2117524*]","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2117524","assignee":"rfredette@redhat.com","components":["Networking"],"product":"OpenShift Container Platform","labels":null,"flags":["qe_test_coverage: ?","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2117524:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:1977660","summary":"the pod events show error codes when crio recreate the missing symlinks","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, a symlinks error message was printed out as raw data instead of formatted as an error, making it difficult to understand. This fix formats the error message properly, so that it is easily understood. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1977660[*BZ#1977660*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=1977660","assignee":"skclark@redhat.com","components":["Node"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 1977660:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2059125","summary":"The oc binary for mac arm64 can’t be executed","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, on macOS arm64 architecture, the `oc` binary needed to be signed manually. As a result, the `oc` binary did not work as expected. This update implements a self-signing binary for `oc` mimicking. As a result, the `oc` binary on macOS arm64 architectures works properly. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2059125[*BZ#2059125*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2059125","assignee":"aguclu@redhat.com","components":["oc"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2059125:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2095708","summary":"oc adm inspect throws out erorr \"the server doesn't have a resource type \"egressfirewalls\" for all operators","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, `must-gather` was trying to collect resources that were not present on the server. Consequently, `must-gather` would print error messages. Now, before collecting resources, `must-gather` checks whether the resource exists. As a result, `must-gather` no longer prints an error when it fails to collect non-existing resources on the server. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2095708[*BZ#2095708*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2095708","assignee":"jchaloup@redhat.com","components":["oc"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2095708:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2097557","summary":"can not upgrade. Incorrect reading of olm.maxOpenShiftVersion","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, Operator Lifecycle Manager (OLM) would attempt to update namespaces to apply a label, even if the label was present on the namespace. Consequently, the update requests increased the workload in API and etcd services. With this update, OLM compares existing labels against the expected labels on a namespace before issuing an update. As a result, OLM no longer attempts to make unnecessary update requests on namespaces. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2105045[*BZ#2105045*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2097557","assignee":"pegoncal@redhat.com","components":["OLM"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2097557:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2105045","summary":"OLM updates namespace labels even if they haven't changed","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, Operator Lifecycle Manager (OLM) would attempt to update namespaces to apply a label, even if the label was present on the namespace. Consequently, the update requests increased the workload in API and etcd services. With this update, OLM compares existing labels against the expected labels on a namespace before issuing an update. As a result, OLM no longer attempts to make unnecessary update requests on namespaces. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2105045[*BZ#2105045*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2105045","assignee":"agreene@redhat.com","components":["OLM"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: +","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2105045:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2091864","summary":"Registry Pod don't have \"securityContext.runAsNonRoot=true\" config that generated by run bundle","description":null,"doc_type":"Bug Fix","doc_text":"* With this update, you can now set the security context for the registry pod by including the `securityContext` configuration field in the pod specification. This will apply the security context for all containers in the pod. The `securityContext` field also defines the pod's privileges. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2091864[*BZ#2091864*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2091864","assignee":"rgottipa@redhat.com","components":["Operator SDK"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2091864:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:1915537","summary":"[Metal] sosreport is broken on a second usage from another debug pod for the same node (BareMetal IPI)","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, the `podman exec` command did not work well with nested containers. Users encountered this issue when accessing a node using the `oc debug` command and then running a container with the `toolbox` command. Because of this, users were unable to reuse toolboxes on {op-system}. This fix updates the toolbox library code to account for this behavior, so users can now reuse toolboxes on {op-system}. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1915537[*BZ#1915537*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=1915537","assignee":"travier@redhat.com","components":["RHCOS"],"product":"OpenShift Container Platform","labels":null,"flags":["reviewed-in-sprint: -","needinfo: ?","requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 1915537:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2048789","summary":"broken toolbox in OCP 4.10 with non-default image","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, updating to Podman 4.0 prevented users from using custom images with toolbox containers on {op-system}. This fix updates the toolbox library code to account for the new Podman behavior, so users can now use custom images with toolbox on {op-system} as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2048789[*BZ#2048789*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2048789","assignee":"travier@redhat.com","components":["RHCOS"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2048789:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2049591","summary":"[RFE] Toolbox - make sure we are running on the latest image?","description":null,"doc_type":"Enhancement","doc_text":"* With this update, running the `toolbox` command now checks for updates to the default image before launching the container. This improves security and provides users with the latest bug fixes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2049591[*BZ#2049591*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"low","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2049591","assignee":"skunkerk@redhat.com","components":["RHCOS"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2049591:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2093040","summary":"unable to start `toolbox` on RHCOS using `podman` 4.0","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, updating to Podman 4.0 prevented users from running the `toolbox` command on {op-system}. This fix updates the toolbox library code to account for the new Podman behavior, so users can now run `toolbox` on {op-system} as expected. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2093040[*BZ#2093040*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2093040","assignee":"travier@redhat.com","components":["RHCOS"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2093040:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2022328","summary":"kube-controller unpublishing volume after maxWaitForUnmountDuration leaves block devices on node in a inconsistent state","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, {product-title} detachec a volume when a Container Storage Interface (CSI) driver was not able to unmount the volume from a node. Detaching a volume without unmount is not allowed by CSI specifications and drivers could enter `undocumented` state. With this update, CSI drivers are detached before unmount only on unhealthy nodes preventing the `undocumented` state. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2049306[*BZ#2049306*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2022328","assignee":"jsafrane@redhat.com","components":["Storage"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2022328:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2057637","summary":"default VolumeSnapshotClass created by the csi-driver-manila-operator does not contain secrets","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, there were missing annotations on the Manila CSI Driver Operator's VolumeSnapshotClass. Consequently, the Manila CSI snapshotter could not locate secrets, and could not create snapshots with the default VolumeSnapshotClass. This update fixes the issue so that secret names and namespace are included in the default VolumeSnapshotClass. As a result, users can now create snapshots in the Manila CSI Driver Operator using the default VolumeSnapshotClass. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2057637[*BZ#2057637*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2057637","assignee":"gouthamr@redhat.com","components":["Storage"],"product":"OpenShift Container Platform","labels":null,"flags":["qe_test_coverage: +","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2057637:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2080449","summary":"[Azure-file CSI Driver] Read/Write permission denied for non-admin user on azure file csi provisioned volume with fsType=ext4,ext3,ext2,xfs","description":null,"doc_type":"Release Note","doc_text":"* Users can now opt into using the experimental VHD feature on Azure File. To opt in, users must specify the `fstype` parameter in a storage class and enable it with `--enable-vhd=true`. If `fstype` is used and the feature is not set to `true`, the volumes will fail to provision. \r\n+\r\nTo opt out of using the VHD feature, remove the `fstype` parameter from your storage class. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2080449[*BZ#2080449*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"medium","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2080449","assignee":"rbednar@redhat.com","components":["Storage"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2080449:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Missing title."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2082773","summary":"[AWS-EBS-CSI-driver-Operator] Generic ephemeral volumes online resize Filesystem type volume stucked at file system resize phase","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, checks for generic ephemeral volumes failed. With this update, checks for expandable volumes now include generic ephemeral volumes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2082773[*BZ#2082773*]","docs_contact":"opayne@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2082773","assignee":"hekumar@redhat.com","components":["Storage"],"product":"OpenShift Container Platform","labels":null,"flags":["needinfo: -","requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2082773:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2108473","summary":"[vSphere CSI driver operator] CSI controller pod restarting constantly","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, if more than one secret was present for vSphere, the vSphere CSI Operator randomly picked a secret and sometimes caused the Operator to restart. With this update, a warning appears when there is more than one secret on the vCenter CSI Operator. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2108473[*BZ#2108473*])","docs_contact":"opayne@redhat.com","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"unspecified","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2108473","assignee":"hekumar@redhat.com","components":["Storage"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2108473:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Bugzilla:2107261","summary":"[WMCO] WMCO endpoints missing after WMCO restart in vSphere","description":null,"doc_type":"Bug Fix","doc_text":"* Previously, restarting the Windows Machine Config Operator (WMCO) in a cluster with running Windows nodes caused the Windows exporter endpoint to be removed. Because of this, each Windows node could not report any metrics data. With this update, the endpoint is retained when the WMCO is started. As a result, metrics data is reported properly after restarting WMCO. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2107261[*BZ#2107261*])","docs_contact":"Missing docs contact","doc_text_status":"Done","status":"CLOSED","is_open":false,"priority":"high","url":"https://bugzilla.redhat.com/show_bug.cgi?id=2107261","assignee":"ssoto@redhat.com","components":["Windows Containers"],"product":"OpenShift Container Platform","labels":null,"flags":["requires_doc_text: +","blocker: -"],"target_releases":[],"subsystems":{"Err":"The pool field has an unexpected structure in bug 2107261:\nObject {}"},"groups":[],"public":true,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":{"Error":"Text in one paragraph."},"target_release":{"Warning":"Check target release."}}],[{"id":"Jira:RHELPLAN-69761","summary":"[Intel] [RHEL9] SGX 1.5 (SGX1 + Flexible Launch Control) support","description":"SGX 1.5 (SGX1 + Flexible Launch Control) support for RHEL 9","doc_type":"Enhancement","doc_text":" ","docs_contact":"jherrman@redhat.com","doc_text_status":"WIP","status":"In Progress","is_open":true,"priority":"Medium","url":"https://issues.redhat.com/browse/RHELPLAN-69761","assignee":"cohuck@redhat.com","components":[],"product":"RHEL Planning","labels":["rhel9_release_note"],"flags":null,"target_releases":["9.2.0"],"subsystems":{"Ok":["sst_virtualization_hwe"]},"groups":null,"public":false,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":{"Error":"RN not approved."},"title_and_text":{"Error":"Empty RN."},"target_release":"Ok"}],[{"id":"Jira:RHELPLAN-139430","summary":"[DOC][SSSD] Add an option to sssd config to convert home directories to lowercase","description":"Made from BZ https://bugzilla.redhat.com/show_bug.cgi?id=1964121\r\n____________________________________________________________\r\n\r\nsssd currently has an option that will allow the administrator to convert back-end directory usernames to lowercase, in order to better integrate with the case-sensitive nature of the Linux environment. \r\n\r\nThere is currently no option to do the same thing for a users defined home directory.  In the case of an active directory back-end, it is not uncommon for user information to be populated in mixed case, this may include the home directory.  Other identity providers that enable AD identity sources account for this by allowing configuration options that convert both usernames and home directories to all lowercase. \r\n\r\nAdding a feature to sssd that will convert home directories to lowercase would help round out that compatibility.","doc_type":"Enhancement","doc_text":".SSSD support for converting home directories to lowercase\r\n\r\nWith this enhancement, you can now configure SSSD to convert user home directories to lowercase. This helps to integrate better with the case-sensitive nature of the RHEL environment. The `override_homedir` option in the `[nss]` section of the `/etc/sssd/sssd.conf` file now recognizes the `%h` template value. If you use `%h` as part of the `override_homedir` definition, SSSD replaces `%h` with the user’s home directory in lowercase.","docs_contact":"lmcgarry@redhat.com","doc_text_status":"Done","status":"Closed","is_open":false,"priority":"Medium","url":"https://issues.redhat.com/browse/RHELPLAN-139430","assignee":"lmcgarry@redhat.com","components":["Documentation"],"product":"RHEL Planning","labels":["rhel_documentation","sst_identity_management","sst_idm_sssd"],"flags":null,"target_releases":["9.2.0","8.8.0"],"subsystems":{"Ok":["sst_identity_management","sst_idm_sssd"]},"groups":null,"public":false,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":"Ok","target_release":"Ok"}],[{"id":"Jira:RHELPLAN-121049","summary":"RN: X.org configuration utilities do not work under Wayland","description":"This ticket stores a release note. See the Documentation tab.","doc_type":"Known Issue","doc_text":".X.org configuration utilities do not work under Wayland\r\n\r\nX.org utilities for manipulating the screen do not work in the Wayland session. Notably, the `xrandr` utility does not work under Wayland due to its different approach to handling, resolutions, rotations, and layout.","docs_contact":"msuchane@redhat.com","doc_text_status":"Done","status":"Closed","is_open":false,"priority":"Medium","url":"https://issues.redhat.com/browse/RHELPLAN-121049","assignee":"msuchane@redhat.com","components":["Documentation"],"product":"RHEL Planning","labels":[],"flags":null,"target_releases":[],"subsystems":{"Ok":["sst_gpu"]},"groups":null,"public":false,"references":[]},{"development":"Ok","doc_type":"Ok","doc_status":"Ok","title_and_text":"Ok","target_release":"Ok"}]],"per_writer_stats":[{"name":"Missing docs contact","total":35,"complete":0,"warnings":0,"incomplete":35},{"name":"jrouth@redhat.com","total":26,"complete":0,"warnings":0,"incomplete":26},{"name":"opayne@redhat.com","total":6,"complete":0,"warnings":0,"incomplete":6},{"name":"bburt@redhat.com","total":6,"complete":0,"warnings":0,"incomplete":6},{"name":"mpytlak@redhat.com","total":4,"complete":0,"warnings":0,"incomplete":4},{"name":"ssiddhar@redhat.com","total":1,"complete":0,"warnings":0,"incomplete":1},{"name":"lmcgarry@redhat.com","total":0,"complete":0,"warnings":0,"incomplete":0},{"name":"msuchane@redhat.com","total":0,"complete":0,"warnings":0,"incomplete":0},{"name":"jherrman@redhat.com","total":0,"complete":0,"warnings":0,"incomplete":0}],"generated_date":"Fri, 03 Feb 2023 11:32:40 +0000"}